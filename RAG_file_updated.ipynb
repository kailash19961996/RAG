{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
       "        num_rows: 10047\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
       "        num_rows: 82326\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
       "        num_rows: 9650\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco', 'v1.1')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Results-Based Accountability is a disciplined...</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...</td>\n",
       "      <td>what is rba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Yes]</td>\n",
       "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...</td>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20-25 minutes]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[$11 to $22 per square foot]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...</td>\n",
       "      <td>price to install tile in shower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Due to symptoms in the body]</td>\n",
       "      <td>{'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...</td>\n",
       "      <td>why conversion observed in body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answers  \\\n",
       "0  [Results-Based Accountability is a disciplined...   \n",
       "1                                              [Yes]   \n",
       "2                                    [20-25 minutes]   \n",
       "3                       [$11 to $22 per square foot]   \n",
       "4                      [Due to symptoms in the body]   \n",
       "\n",
       "                                            passages  \\\n",
       "0  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...   \n",
       "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...   \n",
       "2  {'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...   \n",
       "3  {'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...   \n",
       "4  {'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...   \n",
       "\n",
       "                                               query  \n",
       "0                                        what is rba  \n",
       "1                       was ronald reagan a democrat  \n",
       "2  how long do you need for sydney and surroundin...  \n",
       "3                    price to install tile in shower  \n",
       "4                    why conversion observed in body  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "train_df = train_data.to_pandas()\n",
    "train_df.drop(['query_id', 'query_type', 'wellFormedAnswers'], axis=1, inplace=True)\n",
    "train_df = train_df[:82326]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is rba</td>\n",
       "      <td>Results-Based Accountability is a disciplined ...</td>\n",
       "      <td>[Since 2007, the RBA's outstanding reputation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[In his younger years, Ronald Reagan was a mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>20-25 minutes</td>\n",
       "      <td>[Sydney, New South Wales, Australia is located...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>$11 to $22 per square foot</td>\n",
       "      <td>[In regards to tile installation costs, consum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>Due to symptoms in the body</td>\n",
       "      <td>[Conclusions: In adult body CT, dose to an org...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                                        what is rba   \n",
       "1                       was ronald reagan a democrat   \n",
       "2  how long do you need for sydney and surroundin...   \n",
       "3                    price to install tile in shower   \n",
       "4                    why conversion observed in body   \n",
       "\n",
       "                                             answers  \\\n",
       "0  Results-Based Accountability is a disciplined ...   \n",
       "1                                                Yes   \n",
       "2                                      20-25 minutes   \n",
       "3                         $11 to $22 per square foot   \n",
       "4                        Due to symptoms in the body   \n",
       "\n",
       "                                            passages  \n",
       "0  [Since 2007, the RBA's outstanding reputation ...  \n",
       "1  [In his younger years, Ronald Reagan was a mem...  \n",
       "2  [Sydney, New South Wales, Australia is located...  \n",
       "3  [In regards to tile installation costs, consum...  \n",
       "4  [Conclusions: In adult body CT, dose to an org...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage = []\n",
    "answer = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    x = train_df['answers'][i].tolist()\n",
    "    if len(x)==0:\n",
    "        x = \"-\"\n",
    "    passage.append((train_df['passages'][i]['passage_text']).tolist())\n",
    "    answer.append(x[0])\n",
    "\n",
    "train_df['passages'] = passage\n",
    "train_df['answers'] = answer\n",
    "train_df = train_df[['query', 'answers', 'passages']]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Feminine form of ALEXANDER. In Greek mythology this was a Mycenaean epithet of the goddess Hera, and an alternate name of Cassandra. It was borne by several early Christian saints, and also by the wife of Nicholas II, the last czar of Russia. ',\n",
       " 'However, the program eventually failed in 1963 because of poor executions. By March 1964, Viet Cong had controlled about 40% of the South Vietnamese rural territories including 80% of Phuoc Tuy, 90% of Binh Duong, 90% of Kien Tuong, 90% of Kien Hoa, and 85% of An Xuyen 5 …. Since 1965, the war gradually shifted from guerrilla to conventional war. The numbers of battles in battalion-size or larger increased quickly during 1965 and 1966 6. As villages were not suitable for conventional combats, most of the fightings in this period took place far away from populated areas.',\n",
       " 'The part of the autonomic nervous system originating in the brain stem and the lower part of the spinal cord that, in general, inhibits or opposes the physiological effects of the sympathetic nervous system, as in tending to stimulate digestive secretions, slow the heart, constrict the pupils, and dilate blood vessels.',\n",
       " \"Some car manufacturers may nonetheless choose to use the word coupe or coupe coupé to describe such a, model.e., g The Cadillac coupe De. Ville, alternatively a coupe is distinguished from a-two door sedan by the lack of A b pillar to support the. roof I want to buy a car, but I sometimes see coupes and I don't get what it means. Thanks Everyone! Add your answer. Source. Submit Cancel.\",\n",
       " 'CECA vs FTA. • CECA and FTA are both economic agreements that are intended to boost trade between countries. • While CECA is bilateral, FTA normally involves a group of countries that have geographical and cultural similarities. • Both aim to boost trade by gradual elimination of barriers, quotas and preferences. CECA vs FTA. International trade, though now a days guided by the rules and regulations of World Trade Organization, is not free from protectionism in the form of trade barriers.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = []\n",
    "for i in range(len(train_df)):\n",
    "    x = train_df['passages'][i]\n",
    "    for j in range(len(x)):\n",
    "        unique_docs.append(x[j])\n",
    "print(len(unique_docs))\n",
    "unique_docs = list(set(unique_docs))\n",
    "unique_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained models and tokenizers\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 222903552 || all params: 222903552 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(t5_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(r=16, \n",
    "                        lora_alpha=32, \n",
    "                        target_modules=[\"q\", \"v\"], \n",
    "                        lora_dropout=0.05, \n",
    "                        bias=\"none\", \n",
    "                        task_type=\"SEQ_2_SEQ_LM\")\n",
    "\n",
    "lora_model = get_peft_model(t5_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1769472 || all params: 224673024 || trainable%: 0.7875765272113843\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode queries and retrieve top-k relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding query and finding top 10: 100%|██████████| 82326/82326 [4:31:46<00:00,  5.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset is <__main__.QADataset object at 0x7fef18611510>\n",
      "what is rba Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. \n",
      "Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.\n",
      "0.12032518535852432\n",
      "\n",
      "dataloader is <torch.utils.data.dataloader.DataLoader object at 0x7feef5e73640>\n",
      "('what is rba Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. ', 'what is rba RBA stands for ReBuildable Atomizer. They are rebuildable/reusable versions of the standard disposables items that every vaper knows – cartomizers, atomizers, etc. The goal here is to provide a cheaper, more sustainable (and often more efficient) way to get your vapor.', 'what is rba RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ', 'what is rba vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.', 'what is rba RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'what is rba A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'what is rba Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'what is rba Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.', \"what is rba Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'what is rba A red blood cell (RBC) count is used to determine if a person has normal RBC level present in the body. This test is used to determine if the person has high or low red blood cell count. ')\n",
      "('Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.')\n",
      "tensor([0.1203, 0.1118, 0.0996, 0.0987, 0.0983, 0.0967, 0.0955, 0.0949, 0.0930,\n",
      "        0.0912], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "queries = train_df['query'].tolist()\n",
    "answers = train_df['answers'].tolist()\n",
    "k_value = 10 # Top 10 documents\n",
    "\n",
    "all_docs = unique_docs\n",
    "doc_embeddings = sentence_transformer.encode(all_docs)\n",
    "\n",
    "# Create FAISS index and store document embeddings\n",
    "import faiss\n",
    "index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "query_embeddings = sentence_transformer.encode(queries)\n",
    "\n",
    "# Create a list to store the inputs\n",
    "inputs = []\n",
    "\n",
    "for i in tqdm(range(len(queries)), desc=\"encoding query and finding top 10\"):\n",
    "    query = queries[i]\n",
    "    answer = answers[i]\n",
    "    query_embedding = query_embeddings[i]\n",
    "    topk_doc_scores, topk_doc_indices = index.search(query_embedding.reshape(1, -1), k_value)\n",
    "    top_docs = [all_docs[idx] for idx in topk_doc_indices[0]]\n",
    "\n",
    "    topk_doc_scores = F.softmax(torch.tensor(topk_doc_scores[0]), dim=0)\n",
    "    \n",
    "    for j, doc_idx in enumerate(topk_doc_indices[0]):\n",
    "        inputs.append((f\"{query} {all_docs[doc_idx]}\", answer, topk_doc_scores[j].item()))\n",
    "        # print(inputs[j])\n",
    "    \n",
    "\n",
    "# Define custom dataset and dataloader\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx]\n",
    "\n",
    "dataset = QADataset(inputs)\n",
    "print(f\"\\ndataset is {dataset}\")\n",
    "for i, j, k in dataset:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print(k)\n",
    "    break\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=k_value, shuffle=False)\n",
    "print(f\"\\ndataloader is {dataloader}\")\n",
    "for i, j, k in dataloader:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been saved to 'processed_dataset.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(dataset, 'processed_dataset.pth')\n",
    "print(\"Dataset has been saved to 'processed_dataset.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = torch.load('processed_dataset.pth')\n",
    "# Recreate the DataLoader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=k_value, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82326"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and encode documents\n",
    "# all_docs = [\"Involuntary muscles are muscles that are not controllable consciously.\",\n",
    "#             \"Muscles are good for what \",\n",
    "#             \"Voluntary muscles good \",\n",
    "#             \"paloma did sleep\"]\n",
    "    \n",
    "# all_docs = all_docs\n",
    "# doc_embeddings = sentence_transformer.encode(all_docs)\n",
    "\n",
    "# # Create FAISS index and store document embeddings\n",
    "# import faiss\n",
    "# index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
    "# index.add(doc_embeddings)\n",
    "\n",
    "# # Encode query and retrieve top-k relevant documents\n",
    "# query = \"who slept last night\"\n",
    "# answer = \"it is paloma\"\n",
    "# query_embedding = sentence_transformer.encode([query])[0]\n",
    "# k_value = 10\n",
    "# _, topk_doc_indices = index.search(query_embedding.reshape(1, -1), k_value)\n",
    "# top_docs = [all_docs[idx] for idx in topk_doc_indices[0]]\n",
    "# print(f\"\\ntop_docs are :\")\n",
    "# for i in top_docs:\n",
    "#     print(i)\n",
    "\n",
    "# # Compute softmax scores for top-k documents\n",
    "# topk_doc_scores, _ = index.search(query_embedding.reshape(1, -1), k_value)\n",
    "# topk_doc_scores = F.softmax(torch.tensor(topk_doc_scores[0]), dim=0)\n",
    "# print(f\"\\nsoftmax_scores are \\n{topk_doc_scores}\")\n",
    "\n",
    "# # Prepare input for encoder-decoder model\n",
    "# inputs = []\n",
    "# for i, doc_idx in enumerate(topk_doc_indices[0]):\n",
    "#     inputs.append((f\"{query} {all_docs[doc_idx]}\", answer, topk_doc_scores[i].item()))\n",
    "# print(f\"\\ninputs are :\")\n",
    "# for i in inputs:\n",
    "#     print(i)\n",
    "\n",
    "# # Define custom dataset and dataloader\n",
    "# class QADataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, inputs):\n",
    "#         self.inputs = inputs\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.inputs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.inputs[idx]\n",
    "\n",
    "# dataset = QADataset(inputs)\n",
    "# print(f\"\\ndataset is {dataset}\")\n",
    "# for i, j, k in dataset:\n",
    "#     print(i)\n",
    "#     print(j)\n",
    "#     print(k)\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=k_value, shuffle=True)\n",
    "# print(f\"\\ndataloader is {dataloader}\")\n",
    "# for i, j, k in dataloader:\n",
    "#     print(i)\n",
    "#     print(j)\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t5 = lora_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, labels=None):\n",
    "        outputs = self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,  #causal mask is handled internally\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "model = QAModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 20\n",
    "\n",
    "save_path = '/workspace/RAG/'\n",
    "checkpoint_path = os.path.join(save_path, 'model_checkpoint.pth')\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loss = []\n",
    "# new_loss = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     epoch_model_loss = 0.0\n",
    "    \n",
    "#     progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "    \n",
    "#     for inputs, answer, score in progress_bar:\n",
    "#         inputs_dict = t5_tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "#         input_ids = inputs_dict['input_ids']\n",
    "#         attention_mask = inputs_dict['attention_mask']\n",
    "\n",
    "#         answer_dict = t5_tokenizer(answer, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "#         answer_dict_ids = answer_dict['input_ids']\n",
    "#         answer_dict_attention_mask = answer_dict['attention_mask']\n",
    "\n",
    "#         decoder_inputs = answer_dict_ids[:, :-1].contiguous()\n",
    "#         labels = answer_dict_ids[:, 1:].contiguous()\n",
    "\n",
    "#         score = score.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask, decoder_inputs, labels=labels)  # causal mask is handled internally\n",
    "\n",
    "#         softmaxed_logits = F.softmax((outputs['logits']), dim=2)\n",
    "#         final = score * softmaxed_logits\n",
    "#         final_result = final.sum(dim=0, keepdim=True)\n",
    "\n",
    "#         predictions = final_result.view(-1, final_result.size(-1))\n",
    "#         flat_labels = decoder_inputs[0].view(-1)\n",
    "#         loss = F.cross_entropy(predictions, flat_labels)\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "#         epoch_model_loss += outputs.loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Update tqdm progress bar description with current loss\n",
    "#         progress_bar.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "#     # Print the average loss for the epoch\n",
    "#     print(f\"-\" * 100)\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs} - Average Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs} - Average Model Loss: {epoch_model_loss / len(dataloader):.4f}\")\n",
    "#     print(f\"-\" * 100)\n",
    "\n",
    "#     model_loss.append(epoch_model_loss / len(dataloader))\n",
    "#     new_loss.append(epoch_loss / len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - ----- Our Loss: 9.3805\n",
      "Epoch 1/20 - Model Loss: 22.5662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - ----- Our Loss: 9.3800\n",
      "Epoch 2/20 - Model Loss: 22.6282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - ----- Our Loss: 9.3798\n",
      "Epoch 3/20 - Model Loss: 22.6805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - ----- Our Loss: 9.3796\n",
      "Epoch 4/20 - Model Loss: 22.7374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - ----- Our Loss: 9.3795\n",
      "Epoch 5/20 - Model Loss: 22.7774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - ----- Our Loss: 9.3793\n",
      "Epoch 6/20 - Model Loss: 22.7956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - ----- Our Loss: 9.3789\n",
      "Epoch 7/20 - Model Loss: 22.8107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - ----- Our Loss: 9.3788\n",
      "Epoch 8/20 - Model Loss: 22.8563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - ----- Our Loss: 9.3787\n",
      "Epoch 9/20 - Model Loss: 22.9051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - ----- Our Loss: 9.3787\n",
      "Epoch 10/20 - Model Loss: 22.9455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - ----- Our Loss: 9.3786\n",
      "Epoch 11/20 - Model Loss: 22.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - ----- Our Loss: 9.3786\n",
      "Epoch 12/20 - Model Loss: 23.0385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - ----- Our Loss: 9.3785\n",
      "Epoch 13/20 - Model Loss: 23.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - ----- Our Loss: 9.3785\n",
      "Epoch 14/20 - Model Loss: 23.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - ----- Our Loss: 9.3784\n",
      "Epoch 15/20 - Model Loss: 23.1607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - ----- Our Loss: 9.3784\n",
      "Epoch 16/20 - Model Loss: 23.2070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - ----- Our Loss: 9.3783\n",
      "Epoch 17/20 - Model Loss: 23.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - ----- Our Loss: 9.3783\n",
      "Epoch 18/20 - Model Loss: 23.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - ----- Our Loss: 9.3783\n",
      "Epoch 19/20 - Model Loss: 23.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - ----- Our Loss: 9.3783\n",
      "Epoch 20/20 - Model Loss: 23.3422\n"
     ]
    }
   ],
   "source": [
    "# batch adjusted\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "our_loss = []\n",
    "model_loss = []\n",
    "\n",
    "max_batches_per_epoch = 100  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_model_loss = 0.0\n",
    "    batch_count = 0 \n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "    \n",
    "    for inputs, answer, score in progress_bar:\n",
    "        if batch_count >= max_batches_per_epoch:\n",
    "            break  # Stop processing after max_batches_per_epoch batches\n",
    "\n",
    "        # Tokenize and move inputs to GPU\n",
    "        inputs_dict = t5_tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs_dict['input_ids'].to(device)\n",
    "        attention_mask = inputs_dict['attention_mask'].to(device)\n",
    "\n",
    "        # Tokenize and move answers to GPU\n",
    "        answer_dict = t5_tokenizer(answer, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        answer_dict_ids = answer_dict['input_ids'].to(device)\n",
    "        decoder_inputs = answer_dict_ids[:, :-1].contiguous()\n",
    "        labels = answer_dict_ids[:, 1:].contiguous()\n",
    "\n",
    "        # Ensure score tensor is on GPU\n",
    "        score = score.to(device).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask, decoder_inputs, labels=labels)  # causal mask is handled internally\n",
    "        softmaxed_logits = torch.nn.functional.softmax(outputs.logits, dim=2)\n",
    "        final = score * softmaxed_logits\n",
    "        final_result = final.sum(dim=0, keepdim=True)\n",
    "\n",
    "        # Compute custom loss\n",
    "        predictions = final_result.view(-1, final_result.size(-1))\n",
    "        flat_labels = decoder_inputs[0].view(-1)\n",
    "        loss = torch.nn.functional.cross_entropy(predictions, flat_labels)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_model_loss += outputs.loss.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar and batch count\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "        batch_count += 1\n",
    "\n",
    "    # Save the model state at the end of the epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': epoch_loss\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Record and print average losses for the epoch\n",
    "    our_loss.append(epoch_loss / max_batches_per_epoch)\n",
    "    model_loss.append(epoch_model_loss / max_batches_per_epoch)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - ----- Our Loss: {epoch_loss / max_batches_per_epoch:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Model Loss: {epoch_model_loss / max_batches_per_epoch:.4f}\")\n",
    "    \n",
    "    # model.eval()  # Set model to evaluation mode\n",
    "    # with torch.no_grad():\n",
    "    #     valid_loss = 0.0\n",
    "    #     for data in valid_loader:\n",
    "    #         inputs, labels = data\n",
    "    #         inputs, labels = inputs.to(device), labels.to(device)\n",
    "    #         outputs = model(inputs)\n",
    "    #         loss = criterion(outputs, labels)\n",
    "    #         valid_loss += loss.item()\n",
    "    #     print(f'Validation Loss: {valid_loss / len(valid_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9.380529931640693,\n",
       "  9.380031677870324,\n",
       "  9.37978298384778,\n",
       "  9.379620706042285,\n",
       "  9.379461540351523,\n",
       "  9.379280284365967,\n",
       "  9.378908847911816,\n",
       "  9.378817162271815,\n",
       "  9.378737169139882,\n",
       "  9.378672637359053,\n",
       "  9.378610849779934,\n",
       "  9.378557599470824,\n",
       "  9.378503503734116,\n",
       "  9.378458400716434,\n",
       "  9.378423184752089,\n",
       "  9.378380312340273,\n",
       "  9.378341545593617,\n",
       "  9.378314832661978,\n",
       "  9.378280815871907,\n",
       "  9.378253497962579],\n",
       " [22.566160259246825,\n",
       "  22.628224830627442,\n",
       "  22.68045867919922,\n",
       "  22.737388286590576,\n",
       "  22.777408618927,\n",
       "  22.795561599731446,\n",
       "  22.810731143951415,\n",
       "  22.856279830932618,\n",
       "  22.905063304901123,\n",
       "  22.945488376617433,\n",
       "  22.997629108428956,\n",
       "  23.038520584106447,\n",
       "  23.078159084320067,\n",
       "  23.11859266281128,\n",
       "  23.160689220428466,\n",
       "  23.207026538848876,\n",
       "  23.23833547592163,\n",
       "  23.275337753295897,\n",
       "  23.31565788269043,\n",
       "  23.342175731658937])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_loss, model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loss = []\n",
    "# new_loss = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for inputs, answer, score in dataloader:\n",
    "#         inputs_dict = t5_tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "#         input_ids = inputs_dict['input_ids']\n",
    "#         attention_mask = inputs_dict['attention_mask']\n",
    "\n",
    "#         print(f\"\\ninput_ids are {input_ids.shape}\\n{input_ids}\")\n",
    "#         print(f\"input attention_mask are {attention_mask.shape}\\n{attention_mask}\")\n",
    "\n",
    "#         answer_dict = t5_tokenizer(answer, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "#         answer_dict_ids = answer_dict['input_ids']\n",
    "#         answer_dict_attention_mask = answer_dict['attention_mask']\n",
    "#         print(f\"\\nanaswer_dict are {answer_dict_ids.shape}\\n{answer_dict_ids}\")\n",
    "#         print(f\"anaswer_dict_attention_mask are {answer_dict_attention_mask.shape}\\n{answer_dict_attention_mask}\")\n",
    "        \n",
    "#         print(\"\" + \"-\" * 100)\n",
    "\n",
    "#         decoder_inputs = answer_dict_ids[:, :-1].contiguous()\n",
    "#         print(f\"\\ndecoder_inputs is {decoder_inputs.shape}\\n{decoder_inputs}\")\n",
    "#         labels = answer_dict_ids[:, 1:].contiguous()\n",
    "#         print(f\"labels is {labels.shape}\\n{labels}\")\n",
    "#         score = score.unsqueeze(1).unsqueeze(2)\n",
    "#         print(f\"\\nscore is {score.shape}\\n{score}\")\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask, decoder_inputs, labels=labels) #causal mask is handled internally\n",
    "#         print(\"\\nDecoder done\" + \"-\" * 100)\n",
    "#         print(f\"\\noutputs is {outputs.keys()}\\n\\nloslogitss is {outputs['loss']}\\n\\nlogits is {outputs['logits'].shape}\\n{outputs['logits']}\")\n",
    "#         softmaxed_logits = F.softmax((outputs['logits']), dim=2)\n",
    "#         print(f\"\\nsoftmaxed_logits are {softmaxed_logits.shape}\\n{softmaxed_logits}\")\n",
    "#         print(f\"\\nThe sum of 1st row is {softmaxed_logits[0, 0].sum()}\")\n",
    "#         print(\"\\nPrinted outputs\" + \"-\" * 100)\n",
    "#         final = score * softmaxed_logits\n",
    "#         print(f\"\\nAfter multiplied to softmax are {final.shape}\\n{final}\")\n",
    "#         final_result = final.sum(dim=0, keepdim=True) \n",
    "#         print(f\"\\nAfter summed are {final_result.shape}\\n{final_result}\")\n",
    "#         print(f\"\\nThe sum of 1st row is {final_result[0, 0].sum()}\")\n",
    "#         print(\"\\nMarginalise done\" + \"-\" * 100)\n",
    "#         # loss = outputs.loss # I wont be interested in this loss\n",
    "#         predictions = final_result.view(-1,final_result.size(-1))\n",
    "#         flat_labels = decoder_inputs[0].view(-1)\n",
    "#         print(f\"predictions shape is {predictions.shape}\")\n",
    "#         print(f\"flat_labels shape is {flat_labels.shape}\")\n",
    "#         loss = F.cross_entropy(predictions, flat_labels)\n",
    "#         model_loss.append(outputs.loss)\n",
    "#         new_loss.append(loss)\n",
    "#         print(f\"\\nloss is {loss}\")\n",
    "#         print(f\"\\nmodel loss is {outputs.loss}\")\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         print(f\"\\nEPOCH {epoch} DONE\" + \"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))  \n",
    "plt.plot(our_loss,label='our_loss')\n",
    "plt.plot(model_loss, label='new_loss')  \n",
    "plt.title('Simple Line Plot')  \n",
    "plt.xlabel('X Axis Label')  \n",
    "plt.ylabel('Y Axis Label') \n",
    "plt.legend()  \n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁example', '▁of', '▁in', 'vol', 'unta', 'ry', '▁muscle', '▁tissue', '▁is', '▁Mus', 'cle', 's'] [4273, 6, 25, 33, 269]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer ='example of involuntary muscle tissue is Muscles '\n",
    "tokens = ['▁yes', ',', '▁you', '▁are', '▁right']\n",
    "print(t5_tokenizer.tokenize(answer), t5_tokenizer.convert_tokens_to_ids(tokens))\n",
    "len(['▁example', '▁of', '▁in', 'vol', 'unta', 'ry', '▁muscle', '▁tissue', '▁is', '▁Mus', 'cle', 's'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</s>', '<pad>', 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode(1), t5_tokenizer.decode(0),  t5_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes, you are right</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode([4273,    6,   25,   33,  269,    1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/workspace/RAG/trained_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = QAModel()\n",
    "# model.load_state_dict(torch.load('/workspace/RAG/trained_model.pth'))\n",
    "# model.eval()\n",
    "# model.to(device)\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# def generate_answer(query, max_length=128):\n",
    "#     inputs = tokenizer.encode(query, return_tensors=\"pt\", add_special_tokens=True)\n",
    "#     inputs = inputs.to(device)  # Move inputs to the correct device\n",
    "\n",
    "#     # Generate outputs using the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.t5.generate(input_ids=inputs, \n",
    "#                                     max_length=max_length, \n",
    "#                                     num_beams=5, \n",
    "#                                     no_repeat_ngram_size=2,\n",
    "#                                     early_stopping=True)\n",
    "\n",
    "#     # Decode and return the generated text\n",
    "#     answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return answer\n",
    "\n",
    "# # Example usage:\n",
    "# query = \"What is the capital of France?\"\n",
    "# answer = generate_answer(query)\n",
    "# print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QAModel()\n",
    "model.load_state_dict(torch.load('/workspace/RAG/trained_model.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def generate_answer(query, k_value=10, max_length=12):\n",
    "    query_embedding = sentence_transformer.encode([query])\n",
    "    topk_doc_scores, topk_doc_indices = index.search(query_embedding, k_value)\n",
    "    top_docs = [all_docs[idx] for idx in topk_doc_indices[0]]\n",
    "    # inputs = [(f\"{query} {doc}\") for doc in top_docs]\n",
    "    inputs = query+\" \"+' '.join([doc for doc in top_docs])\n",
    "    print(f\"{inputs}\\n\")\n",
    "    # for i in inputs:\n",
    "    #     print(i)\n",
    "    \n",
    "    # Encode the inputs using the tokenizer\n",
    "    input_ids = t5_tokenizer.batch_encode_plus(inputs, max_length=512, padding=True, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # Generate responses from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.t5.generate(\n",
    "                                    input_ids=input_ids['input_ids'], \n",
    "                                    attention_mask=input_ids['attention_mask'],\n",
    "                                    max_length=max_length, \n",
    "                                    num_beams=5, \n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    early_stopping=True)\n",
    "        print(f\"logits is {outputs.shape}\")\n",
    "        \n",
    "    answers = [t5_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return answers\n",
    "\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "answers = generate_answer(query)\n",
    "for answer in answers:\n",
    "    print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids are torch.Size([10, 193])\n",
      "tensor([[ 363,   19,    8,  ...,    0,    0,    0],\n",
      "        [ 363,   19,    8,  ...,    0,    0,    0],\n",
      "        [ 363,   19,    8,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 363,   19,    8,  ...,    0,    0,    0],\n",
      "        [ 363,   19,    8,  ...,    0,    0,    0],\n",
      "        [ 363,   19,    8,  ..., 2109,    5,    1]], device='cuda:0')\n",
      "attention_mask are torch.Size([10, 193])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')\n",
      "start_token_ids are torch.Size([1])\n",
      "tensor([0], device='cuda:0')\n",
      "decoder_input_ids are torch.Size([1, 1])\n",
      "tensor([[0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1930) must match the size of tensor b (193) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     57\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 59\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m answers:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[0;32mIn[29], line 41\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(query, k_value, max_length)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Generate responses from the model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 41\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(outputs\u001b[38;5;241m.\u001b[39mlogits)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     predicted_ids \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mQAModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, labels)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, decoder_input_ids, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#causal mask is handled internally\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1326\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1325\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1326\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1742\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1739\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1757\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1109\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1095\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1096\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         output_attentions,\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:719\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:630\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    619\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    628\u001b[0m ):\n\u001b[1;32m    629\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 630\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    642\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:555\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    553\u001b[0m     position_bias_masked \u001b[38;5;241m=\u001b[39m position_bias\n\u001b[0;32m--> 555\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[1;32m    556\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[1;32m    557\u001b[0m     scores\n\u001b[1;32m    558\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    559\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    560\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    561\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1930) must match the size of tensor b (193) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "def generate_answer(query, k_value=10, max_length=128):\n",
    "    # Generate the query embedding\n",
    "    query_embedding = sentence_transformer.encode([query])\n",
    "\n",
    "    # Retrieve top-k documents using FAISS\n",
    "    topk_doc_scores, topk_doc_indices = index.search(query_embedding, k_value)\n",
    "    top_docs = [all_docs[idx] for idx in topk_doc_indices[0]]\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    inputs = [f\"{query} {doc}\" for doc in top_docs]\n",
    "    # for i in inputs:\n",
    "    #     print(i)                   \n",
    "\n",
    "    encoded_inputs = t5_tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_inputs['input_ids'].to(device)\n",
    "    attention_mask = encoded_inputs['attention_mask'].to(device)\n",
    "\n",
    "    print(f\"input_ids are {input_ids.shape}\\n{input_ids}\")\n",
    "    print(f\"attention_mask are {attention_mask.shape}\\n{attention_mask}\")\n",
    "\n",
    "    # Prepare decoder input ids with start token\n",
    "    start_token_ids = torch.tensor([tokenizer.encode(\"<pad>\")[0]]).to(device)\n",
    "    decoder_input_ids = start_token_ids.unsqueeze(1)\n",
    "\n",
    "    print(f\"start_token_ids are {start_token_ids.shape}\\n{start_token_ids}\")\n",
    "    print(f\"decoder_input_ids are {decoder_input_ids.shape}\\n{decoder_input_ids}\")\n",
    "\n",
    "    # Generate responses from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, decoder_input_ids)\n",
    "        print(f\"logits {(outputs.logits).shape}\")\n",
    "        predicted_ids = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "    # Decode the generated ids to text for each input\n",
    "    answers = [tokenizer.decode(output, skip_special_tokens=True) for output in predicted_ids]\n",
    "    return answers\n",
    "\n",
    "\n",
    "# Inference\n",
    "model = QAModel()\n",
    "model.load_state_dict(torch.load('/workspace/RAG/trained_model.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of France?\"\n",
    "\n",
    "answers = generate_answer(query)\n",
    "for answer in answers:\n",
    "    print(\"Generated Answer:\", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
