{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU\")\n",
    "    \n",
    "# Load pre-trained models and tokenizers\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1769472 || all params: 224673024 || trainable%: 0.7875765272113843\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(r=16, \n",
    "                        lora_alpha=32, \n",
    "                        target_modules=[\"q\", \"v\"], \n",
    "                        lora_dropout=0.05, \n",
    "                        bias=\"none\", \n",
    "                        task_type=\"SEQ_2_SEQ_LM\")\n",
    "\n",
    "lora_model = get_peft_model(t5_model, lora_config)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is rba</td>\n",
       "      <td>Results-Based Accountability is a disciplined ...</td>\n",
       "      <td>[Since 2007, the RBA's outstanding reputation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[In his younger years, Ronald Reagan was a mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>20-25 minutes</td>\n",
       "      <td>[Sydney, New South Wales, Australia is located...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>$11 to $22 per square foot</td>\n",
       "      <td>[In regards to tile installation costs, consum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>Due to symptoms in the body</td>\n",
       "      <td>[Conclusions: In adult body CT, dose to an org...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0                                        what is rba   \n",
       "1                       was ronald reagan a democrat   \n",
       "2  how long do you need for sydney and surroundin...   \n",
       "3                    price to install tile in shower   \n",
       "4                    why conversion observed in body   \n",
       "\n",
       "                                             answers  \\\n",
       "0  Results-Based Accountability is a disciplined ...   \n",
       "1                                                Yes   \n",
       "2                                      20-25 minutes   \n",
       "3                         $11 to $22 per square foot   \n",
       "4                        Due to symptoms in the body   \n",
       "\n",
       "                                            passages  \n",
       "0  [Since 2007, the RBA's outstanding reputation ...  \n",
       "1  [In his younger years, Ronald Reagan was a mem...  \n",
       "2  [Sydney, New South Wales, Australia is located...  \n",
       "3  [In regards to tile installation costs, consum...  \n",
       "4  [Conclusions: In adult body CT, dose to an org...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco', 'v1.1')\n",
    "\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "train_df = train_data.to_pandas()\n",
    "train_df.drop(['query_id', 'query_type', 'wellFormedAnswers'], axis=1, inplace=True)\n",
    "train_df = train_df[:50] # Limiting size\n",
    "\n",
    "passage = []\n",
    "answer = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    x = train_df['answers'][i].tolist()\n",
    "    if len(x)==0:\n",
    "        x = \"-\"\n",
    "    passage.append((train_df['passages'][i]['passage_text']).tolist())\n",
    "    answer.append(x[0])\n",
    "\n",
    "train_df['passages'] = passage\n",
    "train_df['answers'] = answer\n",
    "train_df = train_df[['query', 'answers', 'passages']]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Enhancement and improvement costs. 1  Polished nickel faucets-Average cost is $400 each plus four hours of installation; 2  Install ceramic tile floor to match shower-Average prices for installation are between $11 to $22 per square foot; 3  A light/fan combination-Averages at $180 and one hour of installation; 4  Insulate and re-finish ceilings and 5  ... Painti',\n",
       " 'There are more than 60 different species of opossum, which are often called possums. The most notable is the Virginia opossum or common opossum—the only marsupial (pouched mammal) found in the United States and Canada. A female opossum gives birth to helpless young as tiny as honeybees. ',\n",
       " 'Our free calculator uses recent, trusted data to estimate costs for your Bathroom Floor Tile Installation project. For a basic 120 square feet project in zip code 47474, the benchmark cost to Install Bathroom Floor Tile ranges between $9.53 - $13.80 per square foot* .',\n",
       " 'The effects of the eruption were felt worldwide. It ejected roughly 10,000,000,000 tonnes (1.1 × 10 10 short tons) or 10 km 3 (2.4 cu mi) of magma, and 20,000,000 tonnes (22,000,000 short tons) SO 2, bringing vast quantities of minerals and metals to the surface environment. It injected more particulate into the stratosphere than any eruption since Krakatoa in 1883. Over the following months, the aerosols formed a global layer of sulfuric acid haze. Global temperatures dropped by about 0.5 °C (0.9 °F) in the years 1991-93, and ozone depletion temporarily increased substantially',\n",
       " 'Dr. Seuss is the pen name of American author, poet and cartoonist Theodor Seuss Geisel (1904-1991). During his career, he wrote nearly 50 children’s books full of imaginative characters and memorable rhymes. He also wrote under the pen names Theo LeSieg and Rosetta Stone. Among the characters he created were The Cat in the Hat, The Grinch, The Lorax, Horton (of “Hears a Who” fame) and many more. Dr. Seuss’ first book was And To Think That I Saw It on Mulberry Street, published in 1937.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = []\n",
    "for i in range(len(train_df)):\n",
    "    x = train_df['passages'][i]\n",
    "    for j in range(len(x)):\n",
    "        unique_docs.append(x[j])\n",
    "print(len(unique_docs))\n",
    "unique_docs = list(set(unique_docs))\n",
    "\n",
    "# Encode all docs\n",
    "all_docs = unique_docs\n",
    "doc_embeddings = sentence_transformer.encode(all_docs)\n",
    "\n",
    "# Create FAISS index and store document embeddings\n",
    "import faiss\n",
    "index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "unique_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding query and finding top 10: 100%|██████████| 50/50 [00:00<00:00, 5363.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dataset is <__main__.QADataset object at 0x7f5151514790>\n",
      "what is rba Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. \n",
      "Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.\n",
      "0.1244548037648201\n",
      "\n",
      "dataloader is <torch.utils.data.dataloader.DataLoader object at 0x7f511dacead0>\n",
      "('what is rba Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. ', 'what is rba RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ', 'what is rba vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.', 'what is rba RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'what is rba A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'what is rba Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'what is rba Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.', \"what is rba Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", \"what is rba The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'what is rba The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.')\n",
      "('Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.', 'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.')\n",
      "tensor([0.1245, 0.1030, 0.1021, 0.1017, 0.1000, 0.0988, 0.0981, 0.0962, 0.0924,\n",
      "        0.0832], dtype=torch.float64)\n",
      "\n",
      "Dataset has been saved to 'processed_dataset.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queries = train_df['query'].tolist()\n",
    "answers = train_df['answers'].tolist()\n",
    "k_value = 10 # Top 10 documents\n",
    "\n",
    "\n",
    "query_embeddings = sentence_transformer.encode(queries)\n",
    "\n",
    "# Create a list to store the inputs\n",
    "inputs = []\n",
    "\n",
    "for i in tqdm(range(len(queries)), desc=\"encoding query and finding top 10\"):\n",
    "    query = queries[i]\n",
    "    answer = answers[i]\n",
    "    query_embedding = query_embeddings[i]\n",
    "    topk_doc_scores, topk_doc_indices = index.search(query_embedding.reshape(1, -1), k_value)\n",
    "    top_docs = [all_docs[idx] for idx in topk_doc_indices[0]]\n",
    "    topk_doc_scores = F.softmax(torch.tensor(topk_doc_scores[0]), dim=0) # finding top scores\n",
    "    for j, doc_idx in enumerate(topk_doc_indices[0]):\n",
    "        inputs.append((f\"{query} {all_docs[doc_idx]}\", answer, topk_doc_scores[j].item()))\n",
    "        # print(inputs[j])\n",
    "    \n",
    "# Define custom dataset and dataloader\n",
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx]\n",
    "\n",
    "dataset = QADataset(inputs)\n",
    "print(f\"\\ndataset is {dataset}\")\n",
    "for i, j, k in dataset:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print(k)\n",
    "    break\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=k_value, shuffle=False)\n",
    "print(f\"\\ndataloader is {dataloader}\")\n",
    "for i, j, k in dataloader:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print(k)\n",
    "    break\n",
    "\n",
    "torch.save(dataset, 'processed_dataset.pth')\n",
    "print(\"\\nDataset has been saved to 'processed_dataset.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = torch.load('/workspace/RAG/RAG/processed_dataset.pth')\n",
    "# Recreate the DataLoader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=k_value, shuffle=False)\n",
    "\n",
    "randy = 0\n",
    "for i,j,k in dataloader:\n",
    "    if randy == 156:\n",
    "        print(i)\n",
    "        print(j)\n",
    "        print(k)\n",
    "        print(k.sum(dim = 0 , keepdims = True))\n",
    "        break\n",
    "    randy +=1\n",
    "\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAModel(\n",
       "  (t5): PeftModelForSeq2SeqLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): T5ForConditionalGeneration(\n",
       "        (shared): Embedding(32128, 768)\n",
       "        (encoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (decoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t5 = lora_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, labels=None):\n",
    "        outputs = self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,  #causal mask is handled internally\n",
    "            labels=labels)\n",
    "        return outputs\n",
    "\n",
    "model = QAModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 20\n",
    "\n",
    "save_path = '/workspace/RAG/'\n",
    "checkpoint_path = os.path.join(save_path, 'small_model.pth')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Our Loss: 10.3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Our Loss: 10.3010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Our Loss: 10.2966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Our Loss: 10.2913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Our Loss: 10.2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Our Loss: 10.2801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Our Loss: 10.2747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Our Loss: 10.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Our Loss: 10.2633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Our Loss: 10.2575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Our Loss: 10.2516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Our Loss: 10.2454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Our Loss: 10.2389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Our Loss: 10.2318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Our Loss: 10.2245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Our Loss: 10.2171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Our Loss: 10.2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Our Loss: 10.2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Our Loss: 10.1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Our Loss: 10.1887\n"
     ]
    }
   ],
   "source": [
    "our_loss = []\n",
    "model_loss = []\n",
    "\n",
    "max_batches_per_epoch = 50 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_model_loss = 0.0\n",
    "    batch_count = 0 \n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "    \n",
    "    for inputs, answer, score in progress_bar:\n",
    "        if batch_count >= max_batches_per_epoch:\n",
    "            break  # Stop processing after max_batches_per_epoch batches\n",
    "\n",
    "        # Tokenize and move inputs to GPU\n",
    "        inputs_dict = t5_tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs_dict['input_ids'].to(device)\n",
    "        attention_mask = inputs_dict['attention_mask'].to(device)\n",
    "\n",
    "        # Tokenize and move answers to GPU\n",
    "        answer_dict = t5_tokenizer(answer, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        answer_dict_ids = answer_dict['input_ids'].to(device)\n",
    "        decoder_inputs = answer_dict_ids[:, :-1].contiguous()\n",
    "        labels = answer_dict_ids[:, 1:].contiguous()\n",
    "\n",
    "        # Ensure score tensor is on GPU\n",
    "        score = score.to(device).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask, decoder_inputs, labels=labels)  # causal mask is handled internally\n",
    "        softmaxed_logits = torch.nn.functional.softmax(outputs.logits, dim=2)\n",
    "        final = score * softmaxed_logits\n",
    "        final_result = final.sum(dim=0, keepdim=True)\n",
    "\n",
    "        # Compute custom loss\n",
    "        predictions = final_result.view(-1, final_result.size(-1))\n",
    "        flat_labels = decoder_inputs[0].view(-1)\n",
    "        loss = torch.nn.functional.cross_entropy(predictions, flat_labels)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_model_loss += outputs.loss.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar and batch count\n",
    "        progress_bar.set_description(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "        batch_count += 1\n",
    "\n",
    "    # Save the model state at the end of the epoch\n",
    "    torch.save({'optimizer_state_dict': optimizer.state_dict()}, checkpoint_path)\n",
    "    \n",
    "    # Record and print average losses for the epoch\n",
    "    our_loss.append(epoch_loss / max_batches_per_epoch)\n",
    "    model_loss.append(epoch_model_loss / max_batches_per_epoch)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Our Loss: {epoch_loss / max_batches_per_epoch:.4f}\")\n",
    "    # print(f\"Epoch {epoch + 1}/{epochs} - Model Loss: {epoch_model_loss / max_batches_per_epoch:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))  \n",
    "plt.plot(our_loss,label='loss')\n",
    "# plt.plot(model_loss, label='new_loss')  \n",
    "plt.title('Simple Line Plot')  \n",
    "plt.xlabel('Epochs')  \n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()  \n",
    "plt.grid(True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QAModel(\n",
       "  (t5): PeftModelForSeq2SeqLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): T5ForConditionalGeneration(\n",
       "        (shared): Embedding(32128, 768)\n",
       "        (encoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (decoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 768)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 12)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-11): 11 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerCrossAttention(\n",
       "                  (EncDecAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (2): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                    (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t5 = lora_model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, decoder_input_ids, labels=None):\n",
    "        outputs = self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,  #causal mask is handled internally\n",
    "            labels=labels)\n",
    "        return outputs\n",
    "\n",
    "model = QAModel()\n",
    "model.load_state_dict(torch.load('/workspace/RAG/small_trained.pth'))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1176, 0.1089, 0.1023, 0.1005, 0.1000, 0.0967, 0.0942, 0.0939, 0.0931,\n",
      "        0.0929])\n",
      "tensor(1.)\n",
      "10\n",
      "\n",
      "dataloader is <torch.utils.data.dataloader.DataLoader object at 0x7f5098328970>\n",
      "(\"translate English to French: What is the capital of Australia? Sydney is the capital city of the Australian state of New South Wales, and Australia's largest city. A week in Sydney will help you see many of the sights of Sydney and its surrounds, and understand the city and its culture. If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\", 'translate English to French: What is the capital of Australia? Sydney, New South Wales, Australia is located in a coastal basin bordered by the Pacific Ocean to the east, the Blue Mountains to the west, the Hawkesbury River to the north and the Woronora Plateau to the south. The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km² (4,689 mi²). This area includes the Central Coast and Blue Mountains as well as broad swathes of national park and other non-urban land.', \"translate English to French: What is the capital of Australia? Sydney lies on a submergent coastline, where the ocean level has risen to flood deep river valleys (rias) carved in the sandstone. There are more than 70 harbour and ocean beaches, including the famous Bondi Beach, in the urban area. Sydney's urban area covers 1,788 km² (690 mi²). The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km² (4,689 mi²). This area includes the Central Coast and Blue Mountains as well as broad swathes of national park and other non-urban land.\", \"translate English to French: What is the capital of Australia? The Sydney central business district, Sydney harbour and outer suburbs from the West. North Sydney 's commercial district. The extensive area covered by urban Sydney is formally divided into more than 300 suburbs for addressing and postal purposes, and administered as 38 local government areas. The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km² (4,689 mi²). This area includes the Central Coast and Blue Mountains as well as broad swathes of national park and other non-urban land.\", 'translate English to French: What is the capital of Australia? The Rocks. This is a complete listing of the suburbs and localities in the greater Sydney area in alphabetical order. Sydney has about 38 local government areas, each consisting of several suburbs (suburbs in Australia are purely geographical, not political, divisions). See table below, Category:Suburbs of Sydney and Category:Local government areas in Sydney. Suburbs are listed here if they are inside the Sydney metro area, and are listed in the Geographical Names Register as being suburbs.', 'translate English to French: What is the capital of Australia? Sydney Attractions. Sydney is home to some of Australia’s most iconic attractions. The Sydney Opera House is a thriving hub of art, culture and history. 1km (return) - 1.5 hour (each way)Step out of your car and into the past. This short walk, perfect for walking with children and for visitors with ties to the local area, pulls you back thro... http://www.nationalparks.nsw.gov.au/scheyville-national-park/migrant-heritage-walk/walking.', 'translate English to French: What is the capital of Australia? Confidence votes 242K. Passionate about all things Australian. Possums and opossums are mammals, specifically marsupials, as their young develop in the pouch. Possums are native to Australia, New Guinea and Sulawesi, whilst opossums are native to North America. They are quite different animals. ', \"translate English to French: What is the capital of Australia? Sydney is a very large city, and we haven't spent much time outside of the inner suburbs, apart from what you would have seen on the way to the Blue Mountains. On Day 5 we'll explore one of Sydney's parkland areas in the Sydney Olympic Park at Homebush Bay [63] . If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\", 'translate English to French: What is the capital of Australia? This itinerary will have you crossing the country to take in the Great Barrier Reef, Australia’s iconic reef in Queensland, before heading to Western Australia to see the breath taking Ningaloo Reef, one of Australia’s best kept secrets. View more information. 10 day-Sydney, rock and reef. It’s easy to see why Hamilton Island is one of the most popular spots for a getaway on the Great Barrier Reef. With palm-fringed beaches, top restaurants and stylish resorts, there’s plenty to do on land, while those keen to explore the clear waters of the Whitsundays will be richly rewarded.', \"translate English to French: What is the capital of Australia? On your right across College Street, in the sandstone building on the corner, is the Australian Museum [3] ($12 adult/$6 children, $30 family (2+2)). This museum, which focuses on natural history, is worth a visit in its own right if you have more time in Sydney and will take a couple of hours to explore. If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\")\n",
      "('Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world', 'Sydney hello hello world')\n",
      "tensor([0.1176, 0.1089, 0.1023, 0.1005, 0.1000, 0.0967, 0.0942, 0.0939, 0.0931,\n",
      "        0.0929], dtype=torch.float64)\n",
      "tensor(1.0000, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "\n",
    "def generate_answer(query, starting_word):\n",
    "    # Generate the query embedding\n",
    "    query_embedding = sentence_transformer.encode([query])\n",
    "\n",
    "    # Retrieve top-k documents using FAISS\n",
    "    topk_doc_scores, topk_doc_indices = index.search(query_embedding, k_value)\n",
    "    topk_doc_scores = F.softmax(torch.tensor(topk_doc_scores[0]), dim=0)\n",
    "    top_docs = [all_docs[idx] for idx in topk_doc_indices[0]]\n",
    "    print(topk_doc_scores)\n",
    "    print(topk_doc_scores.sum(dim=0))\n",
    "\n",
    "    for j, doc_idx in enumerate(topk_doc_indices[0]):\n",
    "        inputs.append((f\"{query} {all_docs[doc_idx]}\", starting_word, topk_doc_scores[j].item()))\n",
    "        # print(inputs[j])\n",
    "    return inputs\n",
    "\n",
    "query = \"translate English to French: What is the capital of Australia?\"\n",
    "starting_word = \"Sydney hello hello world\"\n",
    "ending_word = \"</sos>\"\n",
    "max_length = 50\n",
    "generated_sequence = generate_answer(query, starting_word)\n",
    "print(len(generated_sequence))\n",
    "\n",
    "dataset = QADataset(generated_sequence)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=k_value, shuffle=False)\n",
    "print(f\"\\ndataloader is {dataloader}\")\n",
    "for i, j, k in dataloader:\n",
    "    print(i)\n",
    "    print(j)\n",
    "    print(k)\n",
    "    print(k.sum(dim=0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids are torch.Size([10, 158])\n",
      "tensor([[13959,  1566,    12,  ...,     0,     0,     0],\n",
      "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
      "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
      "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
      "        [13959,  1566,    12,  ...,     0,     0,     0]], device='cuda:0')\n",
      "input attention_mask are torch.Size([10, 158])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
      "\n",
      "answer_dict_ids are torch.Size([10, 5])\n",
      "tensor([[ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1],\n",
      "        [ 7476, 21820, 21820,   296,     1]], device='cuda:0')\n",
      "decoder_inputs are torch.Size([10, 4])\n",
      "tensor([[ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296],\n",
      "        [ 7476, 21820, 21820,   296]], device='cuda:0')\n",
      "\n",
      "score is torch.Size([10, 1, 1])\n",
      "tensor([[[0.1061]],\n",
      "\n",
      "        [[0.1018]],\n",
      "\n",
      "        [[0.1018]],\n",
      "\n",
      "        [[0.1016]],\n",
      "\n",
      "        [[0.1007]],\n",
      "\n",
      "        [[0.0995]],\n",
      "\n",
      "        [[0.0983]],\n",
      "\n",
      "        [[0.0983]],\n",
      "\n",
      "        [[0.0962]],\n",
      "\n",
      "        [[0.0958]]], device='cuda:0', dtype=torch.float64)\n",
      "\n",
      "outputs is torch.Size([10, 4, 32128])\n",
      "\n",
      "softmaxed_logits are torch.Size([10, 4, 32128])\n",
      "\n",
      "The sum of 1st row is 0.9999999403953552\n",
      "\n",
      "After multiplied to softmax are torch.Size([10, 4, 32128])\n",
      "\n",
      "After summed are torch.Size([1, 4, 32128])\n",
      "\n",
      "The sum of 1st row is 1.000000100666519\n",
      "\n",
      "predicted_token is tensor([[    3,     3, 21820, 21820]], device='cuda:0')\n",
      "predicted_word is hello hello\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "\n",
    "for inputs, answer, score in progress_bar:\n",
    "\n",
    "        # Tokenize and move inputs to GPU\n",
    "        inputs_dict = t5_tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs_dict['input_ids'].to(device)\n",
    "        attention_mask = inputs_dict['attention_mask'].to(device)\n",
    "\n",
    "        print(f\"\\ninput_ids are {input_ids.shape}\\n{input_ids}\")\n",
    "        print(f\"input attention_mask are {attention_mask.shape}\\n{attention_mask}\")\n",
    "\n",
    "        # Tokenize and move answers to GPU\n",
    "        answer_dict = t5_tokenizer(answer, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        answer_dict_ids = answer_dict['input_ids'].to(device)\n",
    "        decoder_inputs = answer_dict_ids[:, :-1].contiguous()\n",
    "\n",
    "        print(f\"\\nanswer_dict_ids are {answer_dict_ids.shape}\")\n",
    "        print(f\"decoder_inputs are {decoder_inputs.shape}\")\n",
    "\n",
    "        # Ensure score tensor is on GPU\n",
    "        score = score.to(device).unsqueeze(1).unsqueeze(2)\n",
    "        print(f\"\\nscore is {score.shape}\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask, decoder_inputs)\n",
    "        print(outputs.keys())\n",
    "\n",
    "        print(f\"\\noutputs is {outputs['logits'].shape}\")\n",
    "        softmaxed_logits = torch.nn.functional.softmax(outputs.logits, dim=2)\n",
    "        print(f\"\\nsoftmaxed_logits are {softmaxed_logits.shape}\")\n",
    "        print(f\"\\nThe sum of 1st row is {softmaxed_logits[0, 0].sum()}\")\n",
    "        final = score * softmaxed_logits\n",
    "        print(f\"\\nAfter multiplied to softmax are {final.shape}\")\n",
    "        final_result = final.sum(dim=0, keepdim=True)\n",
    "        print(f\"\\nAfter summed are {final_result.shape}\")\n",
    "        print(f\"\\nThe sum of 1st row is {final_result[0, 0].sum()}\")\n",
    "        predicted_token = torch.argmax(final_result, dim=-1)\n",
    "        print(f\"\\npredicted_token is {predicted_token}\")\n",
    "        predicted_word = t5_tokenizer.decode(predicted_token[-1])\n",
    "        print(f\"predicted_word is {predicted_word}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "class PeftModelForSeq2SeqLM(nn.Module):\n",
    "    def generate(self, input_ids, max_length=20, **kwargs):\n",
    "        # Your implementation here, possibly calling the base generate with necessary arguments\n",
    "        return super().generate(input_ids, max_length=max_length, **kwargs)\n",
    "\n",
    "\n",
    "query = \"translate English to French: hello are you?\"\n",
    "input_ids = t5_tokenizer.encode(query, return_tensors=\"pt\").to(device)  # Convert query to input IDs and transfer to appropriate device\n",
    "\n",
    "# Generate the output IDs from the model\n",
    "output_ids = lora_model.base_model.generate(input_ids, max_length=50)\n",
    "\n",
    "\n",
    "# Decode the output IDs to a string\n",
    "output_text = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
